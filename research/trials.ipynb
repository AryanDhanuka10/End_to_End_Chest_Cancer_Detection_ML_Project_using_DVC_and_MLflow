{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90eff6ba",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02f377a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e25d2428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/aryan-dhanuka/Backup/Python Data Science/AI Projects/End_to_End_Chest_Cancer_Detection_ML_Project_using_DVC_and_MLflow/research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e174fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54d460a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Entity Creation\n",
    "## Entity is the return type or the configuration that is to be returned from the config.yaml\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir : Path\n",
    "    source_URL : str\n",
    "    local_data_file : Path\n",
    "    unzip_dir : Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec535807",
   "metadata": {},
   "source": [
    "##### we read our config.yaml and params.yaml file in constant (init.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a7568b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing both files\n",
    "from cnnClassifier.constants import *\n",
    "from cnnClassifier.utils.common import read_yaml, create_directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50aa0205",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH\n",
    "        ):\n",
    "\n",
    "        # Reading YAML files\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        \n",
    "        # Creating Artifacts Folder\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "            config = self.config.data_ingestion # Imp\n",
    "\n",
    "            create_directories([config.root_dir])\n",
    "\n",
    "            data_ingestion_config = DataIngestionConfig(\n",
    "                root_dir = config.root_dir ,   # Data Ingestion Folder\n",
    "                source_URL = config.source_URL,   # URL of the dataset\n",
    "                local_data_file = config.local_data_file, # the file storage location locally\n",
    "                unzip_dir = config.unzip_dir # unzip data location\n",
    "            )\n",
    "            return data_ingestion_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c6913eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request as request\n",
    "import zipfile\n",
    "import gdown\n",
    "from cnnClassifier import logger\n",
    "from cnnClassifier.utils.common import get_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb7a3968",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update the Components\n",
    "#  updating the data ingestion components\n",
    "class DataIngestion:\n",
    "    def __init__(self, config: DataIngestionConfig):\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    # Download Data\n",
    "    def download_file(self) -> str:\n",
    "        \"\"\"\n",
    "            Fetch the data from the url \n",
    "            \"\"\"\n",
    "        try:\n",
    "            dataset_url = self.config.source_URL\n",
    "            zip_download_dir = self.config.local_data_file # zip file download\n",
    "            os.makedirs(\"artifacts/data_ingestion\", exist_ok=True)\n",
    "            logger.info(f\"Downloading data from {dataset_url} into file {zip_download_dir}\")\n",
    "\n",
    "            file_id = dataset_url.split(\"/\")[-2]\n",
    "            prefix = 'https://drive.google.com/uc?/export=download&id='\n",
    "            gdown.download(prefix+file_id,zip_download_dir)\n",
    "\n",
    "            logger.info(f\"Download data from {dataset_url} into file {zip_download_dir}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "\n",
    "        # Unzip the data\n",
    "    def extract_zip_file(self):\n",
    "        \"\"\"\n",
    "        zip_file_path: str\n",
    "        Extracts the zip file into the  data directory\n",
    "        Function returns None \n",
    "        \"\"\"\n",
    "        unzip_path = self.config.unzip_dir\n",
    "        os.makedirs(unzip_path, exist_ok=True)\n",
    "        with zipfile.ZipFile(self.config.local_data_file, 'r') as zip_ref:\n",
    "            zip_ref.extractall(unzip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29e0413b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-25 19:30:27,794: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2025-09-25 19:30:27,795: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-09-25 19:30:27,796: INFO: common: created directory at: artifacts]\n",
      "[2025-09-25 19:30:27,796: INFO: common: created directory at: artifacts/data_ingestion]\n",
      "[2025-09-25 19:30:27,797: INFO: 1686596915: Downloading data from https://drive.google.com/file/d/1eJyF0daLaHgnWmT0ydhNDqykm97saYWi/view?usp=sharing into file artifacts/data_ingestion/data.zip]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?/export=download&id=1eJyF0daLaHgnWmT0ydhNDqykm97saYWi\n",
      "From (redirected): https://drive.google.com/uc?%2Fexport=download&id=1eJyF0daLaHgnWmT0ydhNDqykm97saYWi&confirm=t&uuid=60fb463c-ae0b-42e6-8058-81ed720db29d\n",
      "To: /home/aryan-dhanuka/Backup/Python Data Science/AI Projects/End_to_End_Chest_Cancer_Detection_ML_Project_using_DVC_and_MLflow/artifacts/data_ingestion/data.zip\n",
      "100%|██████████| 124M/124M [05:09<00:00, 402kB/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-25 19:35:40,251: INFO: 1686596915: Download data from https://drive.google.com/file/d/1eJyF0daLaHgnWmT0ydhNDqykm97saYWi/view?usp=sharing into file artifacts/data_ingestion/data.zip]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Updating the Pipeline\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_ingestion_config = config.get_data_ingestion_config()\n",
    "    data_ingestion = DataIngestion(config=data_ingestion_config)\n",
    "    data_ingestion.download_file()\n",
    "    data_ingestion.extract_zip_file()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd61b458",
   "metadata": {},
   "source": [
    "### Prepare Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6095606",
   "metadata": {},
   "source": [
    "We will apply transfer learning where we will make changes in the last(Dense) layer of VGG - 16 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b121c1a",
   "metadata": {},
   "source": [
    "#### All libraries are imported above ,no  need to re import again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8278dd32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/aryan-dhanuka/Backup/Python Data Science/AI Projects/End_to_End_Chest_Cancer_Detection_ML_Project_using_DVC_and_MLflow'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84052c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class PrepareBaseModelConfig:\n",
    "    root_dir : Path\n",
    "    base_model_path : Path\n",
    "    updated_model_path : Path\n",
    "    params_image_size : list\n",
    "    params_learning_rate : float\n",
    "    params_include_top : bool\n",
    "    params_weights : str\n",
    "    params_classes : int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18c60f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH\n",
    "        ):\n",
    "\n",
    "        # Reading YAML files\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        \n",
    "        # Creating Artifacts Folder\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_prepare_base_model_config(self) -> PrepareBaseModelConfig:\n",
    "            config = self.config.prepare_base_model # Imp\n",
    "\n",
    "            create_directories([config.root_dir])\n",
    "\n",
    "            prepare_base_model_config = PrepareBaseModelConfig(\n",
    "                root_dir = Path(config.root_dir),\n",
    "                base_model_path = Path(config.base_model_path),\n",
    "                updated_model_path = Path(config.updated_base_model_path),\n",
    "                params_image_size = self.params.IMAGE_SIZE,\n",
    "                params_learning_rate =  self.params.LEARNING_RATE,\n",
    "                params_include_top =  self.params.INCLUDE_TOP,\n",
    "                params_weights =  self.params.WEIGHTS,\n",
    "                params_classes =  self.params.CLASSES,\n",
    "            )\n",
    "            return prepare_base_model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3d5513e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 19:35:40.985614: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-25 19:35:41.010696: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3179e462",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareBaseModel:\n",
    "    def __init__(self, config: PrepareBaseModelConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def get_base_model(self):\n",
    "        self.model = tf.keras.applications.vgg16.VGG16(\n",
    "            input_shape = self.config.params_image_size,\n",
    "            weights = self.config.params_weights,\n",
    "            include_top = self.config.params_include_top\n",
    "        )\n",
    "\n",
    "        self.model.save(self.config.base_model_path)\n",
    "    \n",
    "    \n",
    "    def _prepare_full_model(model, classes, freeze_all, freeze_till, learning_rate):\n",
    "        if freeze_all: # freeze all layers if true\n",
    "            for layer in model.layers:\n",
    "                model.trainable = False\n",
    "        elif (freeze_till is not None) and (freeze_till > 0): # freeze till freeze only till the layer mentioned in the argument\n",
    "            for layer in model.layers[:-freeze_till]:\n",
    "                model.trainable = False\n",
    "\n",
    "        flatten_in = tf.keras.layers.Flatten()(model.output)\n",
    "        prediction = tf.keras.layers.Dense(\n",
    "            units=classes,\n",
    "            activation=\"softmax\"\n",
    "        )(flatten_in)\n",
    "\n",
    "        full_model = tf.keras.models.Model(\n",
    "            inputs=model.input,\n",
    "            outputs=prediction\n",
    "        )\n",
    "\n",
    "        full_model.compile(\n",
    "            optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "\n",
    "        full_model.summary()\n",
    "        return full_model\n",
    "    \n",
    "    def update_base_model(self):\n",
    "        self.full_model = PrepareBaseModel._prepare_full_model(\n",
    "            model = self.model,\n",
    "            classes = self.config.params_classes,\n",
    "            freeze_all = True,\n",
    "            freeze_till = None,\n",
    "            learning_rate = self.config.params_learning_rate\n",
    "        )\n",
    "\n",
    "        PrepareBaseModel.save_model(path=self.config.updated_model_path, model = self.full_model)\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(path:Path, model:tf.keras.Model):\n",
    "        model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4af0d8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-25 19:35:41,929: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2025-09-25 19:35:41,931: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-09-25 19:35:41,932: INFO: common: created directory at: artifacts]\n",
      "[2025-09-25 19:35:41,932: INFO: common: created directory at: artifacts/prepare_base_model]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 19:35:41.962339: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-25 19:35:41.983492: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-25 19:35:41.986207: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-25 19:35:41.989562: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-25 19:35:41.992028: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-25 19:35:41.994270: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-25 19:35:42.073702: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-25 19:35:42.074730: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-25 19:35:42.075659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-25 19:35:42.076511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6168 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-25 19:35:42,334: WARNING: saving_utils: Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.]\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 100356    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14815044 (56.51 MB)\n",
      "Trainable params: 100356 (392.02 KB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aryan-dhanuka/miniconda3/envs/tf_gpu/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    prepare_base_model_config = config.get_prepare_base_model_config()\n",
    "    prepare_base_model = PrepareBaseModel(config=prepare_base_model_config)\n",
    "    prepare_base_model.get_base_model()\n",
    "    prepare_base_model.update_base_model()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c623de81",
   "metadata": {},
   "source": [
    "### Model Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63e0aeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    root_dir : Path\n",
    "    trained_model_path : Path\n",
    "    updated_model_path : Path\n",
    "    training_data : Path\n",
    "    validation_data: Path  \n",
    "    params_epochs : int\n",
    "    params_batch_size : int\n",
    "    params_is_augmentation : bool\n",
    "    params_image_size : list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7f860dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnnClassifier.constants import *\n",
    "from cnnClassifier.utils.common import read_yaml, create_directories\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36547b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH\n",
    "        ):\n",
    "\n",
    "        # Reading YAML files\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        \n",
    "        # Creating Artifacts Folder\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        training = self.config.training\n",
    "        prepare_base_model = self.config.prepare_base_model\n",
    "        params = self.params\n",
    "        training_data = os.path.join(self.config.data_ingestion.unzip_dir, \"Data/train\")\n",
    "        validation_data = os.path.join(self.config.data_ingestion.unzip_dir, \"Data/test\")  # Fixed validation path\n",
    "\n",
    "        create_directories([Path(training.root_dir)])\n",
    "\n",
    "        \n",
    "        training_config = TrainingConfig(\n",
    "            root_dir = Path(training.root_dir),\n",
    "            trained_model_path = Path(training.trained_model_path),\n",
    "            updated_model_path = Path(prepare_base_model.updated_base_model_path),\n",
    "            training_data = Path(training_data),\n",
    "            validation_data = Path(validation_data), \n",
    "            params_epochs = params.EPOCHS,\n",
    "            params_batch_size = params.BATCH_SIZE,\n",
    "            params_is_augmentation = params.AUGMENTATION,\n",
    "            params_image_size = params.IMAGE_SIZE\n",
    "        )\n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0698885",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.model = None \n",
    "\n",
    "    def get_base_model(self):\n",
    "        self.model = tf.keras.models.load_model(self.config.updated_model_path, compile=False)\n",
    "        self.model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),  # Reset optimizer\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "    def train_valid_generator(self):\n",
    "        datagenerator_kwargs = dict(\n",
    "            rescale=1./255,\n",
    "            validation_split=0.20\n",
    "        )\n",
    "\n",
    "        dataflow_kwargs = dict(\n",
    "            target_size=self.config.params_image_size[:-1],\n",
    "            batch_size=self.config.params_batch_size,\n",
    "            interpolation=\"bilinear\"\n",
    "        )\n",
    "\n",
    "        # Validation data generator\n",
    "        valid_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(**datagenerator_kwargs)\n",
    "\n",
    "        # self.valid_generator = valid_datagenerator.flow_from_directory(\n",
    "        #     directory=self.config.validation_data,\n",
    "        #     target_size=self.config.params_image_size[:-1],\n",
    "        #     batch_size=self.config.params_batch_size,\n",
    "        #     class_mode=\"categorical\",  # Fixed: Ensures multi-class classification\n",
    "        #     shuffle=False\n",
    "        # )\n",
    "\n",
    "        self.valid_generator = valid_datagenerator.flow_from_directory(\n",
    "            directory=self.config.validation_data,\n",
    "            **dataflow_kwargs,  # UPDATED: Used **dataflow_kwargs to simplify\n",
    "            class_mode=\"categorical\",  \n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        # Training data generator\n",
    "        if self.config.params_is_augmentation:\n",
    "            train_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                rotation_range=40,\n",
    "                horizontal_flip=True,\n",
    "                width_shift_range=0.2,\n",
    "                height_shift_range=0.2,\n",
    "                shear_range=0.2,\n",
    "                zoom_range=0.2,\n",
    "                **datagenerator_kwargs\n",
    "                \n",
    "            )\n",
    "        else:\n",
    "            train_datagenerator = valid_datagenerator\n",
    "\n",
    "        # self.train_generator = train_datagenerator.flow_from_directory(\n",
    "        #     directory=self.config.training_data,\n",
    "        #     target_size=self.config.params_image_size[:-1],\n",
    "        #     batch_size=self.config.params_batch_size,\n",
    "        #     class_mode=\"categorical\",  # Fixed: Ensures correct class label handling\n",
    "        #     shuffle=True\n",
    "        # )\n",
    "\n",
    "        self.train_generator = train_datagenerator.flow_from_directory(\n",
    "            directory=self.config.training_data,\n",
    "            **dataflow_kwargs,  # UPDATED: Used **dataflow_kwargs\n",
    "            class_mode=\"categorical\",\n",
    "            shuffle=True\n",
    "        )\n",
    "        print(\"Train Class Indices:\", self.train_generator.class_indices)\n",
    "        print(\"Train Num Classes:\", self.train_generator.num_classes)\n",
    "        print(\"Valid Class Indices:\", self.valid_generator.class_indices)\n",
    "        print(\"Valid Num Classes:\", self.valid_generator.num_classes)\n",
    "\n",
    "    \n",
    "    def save_model(self,path: Path, model: tf.keras.Model):\n",
    "        model.save(path)\n",
    "\n",
    "    def train(self):\n",
    "        self.steps_per_epoch = self.train_generator.samples // self.train_generator.batch_size\n",
    "        self.validation_steps = self.valid_generator.samples // self.valid_generator.batch_size\n",
    "\n",
    "        if not hasattr(self.model, \"optimizer\"):\n",
    "            self.model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),  # Reset optimizer\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "\n",
    "        self.model.fit(\n",
    "            self.train_generator,\n",
    "            epochs=self.config.params_epochs,\n",
    "            steps_per_epoch=self.steps_per_epoch,\n",
    "            validation_steps=self.validation_steps,\n",
    "            validation_data=self.valid_generator\n",
    "        )\n",
    "\n",
    "        self.save_model(\n",
    "            path=self.config.trained_model_path,\n",
    "            model=self.model\n",
    "        )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "225b74dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-25 19:35:42,583: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2025-09-25 19:35:42,584: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-09-25 19:35:42,585: INFO: common: created directory at: artifacts]\n",
      "[2025-09-25 19:35:42,585: INFO: common: created directory at: artifacts/training]\n",
      "Found 315 images belonging to 4 classes.\n",
      "Found 613 images belonging to 4 classes.\n",
      "Train Class Indices: {'adenocarcinoma_left.lower.lobe_T2_N0_M0_Ib': 0, 'large.cell.carcinoma_left.hilum_T2_N2_M0_IIIa': 1, 'normal': 2, 'squamous.cell.carcinoma_left.hilum_T1_N2_M0_IIIa': 3}\n",
      "Train Num Classes: 4\n",
      "Valid Class Indices: {'adenocarcinoma': 0, 'large.cell.carcinoma': 1, 'normal': 2, 'squamous.cell.carcinoma': 3}\n",
      "Valid Num Classes: 4\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 19:35:43.318039: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/38 [..............................] - ETA: 1:21 - loss: 1.6983 - accuracy: 0.1875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 19:35:44.967935: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2025-09-25 19:35:44.978567: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7da33003ff80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-09-25 19:35:44.978592: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9\n",
      "2025-09-25 19:35:44.981071: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-09-25 19:35:45.052391: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 7s 141ms/step - loss: 1.4286 - accuracy: 0.4791 - val_loss: 1.1832 - val_accuracy: 0.4704\n",
      "Epoch 2/10\n",
      "38/38 [==============================] - 5s 131ms/step - loss: 0.9797 - accuracy: 0.5729 - val_loss: 1.1626 - val_accuracy: 0.6086\n",
      "Epoch 3/10\n",
      "38/38 [==============================] - 5s 131ms/step - loss: 0.9635 - accuracy: 0.6131 - val_loss: 0.7790 - val_accuracy: 0.6151\n",
      "Epoch 4/10\n",
      "38/38 [==============================] - 5s 126ms/step - loss: 0.7163 - accuracy: 0.6817 - val_loss: 0.8165 - val_accuracy: 0.6118\n",
      "Epoch 5/10\n",
      "38/38 [==============================] - 5s 128ms/step - loss: 0.6819 - accuracy: 0.7069 - val_loss: 0.8156 - val_accuracy: 0.6217\n",
      "Epoch 6/10\n",
      "38/38 [==============================] - 5s 128ms/step - loss: 0.6634 - accuracy: 0.7437 - val_loss: 0.7064 - val_accuracy: 0.6809\n",
      "Epoch 7/10\n",
      "38/38 [==============================] - 5s 128ms/step - loss: 0.6142 - accuracy: 0.7404 - val_loss: 0.7124 - val_accuracy: 0.6645\n",
      "Epoch 8/10\n",
      "38/38 [==============================] - 5s 129ms/step - loss: 0.6887 - accuracy: 0.7119 - val_loss: 1.6777 - val_accuracy: 0.6020\n",
      "Epoch 9/10\n",
      "38/38 [==============================] - 5s 128ms/step - loss: 0.8128 - accuracy: 0.6750 - val_loss: 0.7429 - val_accuracy: 0.6809\n",
      "Epoch 10/10\n",
      "38/38 [==============================] - 5s 130ms/step - loss: 0.6504 - accuracy: 0.7219 - val_loss: 0.8257 - val_accuracy: 0.6414\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    training_config = config.get_training_config()\n",
    "    training = Training(config=training_config)\n",
    "    \n",
    "    training.get_base_model()\n",
    "    training.train_valid_generator()\n",
    "    training.train()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26edc5f",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90bd10a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # loads .env file\n",
    "\n",
    "MLFLOW_TRACKING_URI = os.getenv(\"MLFLOW_TRACKING_URI\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bef3a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"artifacts/training/model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56011b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class EvaluationConfig:\n",
    "    def __init__(self, path_of_model, mlflow_url, all_params, params_image_size, params_batch_size, training_data):\n",
    "        self.path_of_model = path_of_model\n",
    "        self.mlflow_url = mlflow_url\n",
    "        self.all_params = all_params\n",
    "        self.params_image_size = params_image_size\n",
    "        self.params_batch_size = params_batch_size\n",
    "        self.training_data = training_data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4d3a4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH\n",
    "        ):\n",
    "\n",
    "        # Reading YAML files\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        \n",
    "        # Creating Artifacts Folder\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_evaluated_config(self) -> EvaluationConfig:\n",
    "        eval_config = EvaluationConfig(\n",
    "            path_of_model=\"artifacts/training/model.h5\",\n",
    "            training_data=\"artifacts/data_ingestion/Data/train\",  # now valid\n",
    "            mlflow_url=MLFLOW_TRACKING_URI,\n",
    "            all_params=self.params,\n",
    "            params_image_size=self.params.IMAGE_SIZE,\n",
    "            params_batch_size=self.params.BATCH_SIZE\n",
    "        )\n",
    "        return eval_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25bbf8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "import tensorflow as tf\n",
    "\n",
    "class Evaluation:\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        config: an object with attributes\n",
    "            - path_of_model: Path to saved Keras model\n",
    "            - training_data: Path to training/validation dataset\n",
    "            - all_params: dict of hyperparameters\n",
    "            - params_image_size: image size tuple\n",
    "            - params_batch_size: batch size\n",
    "            - mlflow_url: MLflow tracking URI\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.valid_generator = None\n",
    "        self.score = None\n",
    "        \n",
    "        mlflow.set_tracking_uri(config.mlflow_url)\n",
    "\n",
    "    def _valid_generator(self):\n",
    "        \"\"\"Create a validation data generator.\"\"\"\n",
    "        datagenerator_kwargs = dict(\n",
    "            rescale=1.0 / 255,\n",
    "            validation_split=0.30\n",
    "        )\n",
    "\n",
    "        dataflow_kwargs = dict(\n",
    "            target_size=self.config.params_image_size[:-1],\n",
    "            batch_size=self.config.params_batch_size,\n",
    "            interpolation=\"bilinear\",\n",
    "            class_mode=\"categorical\"\n",
    "        )\n",
    "\n",
    "        valid_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            **datagenerator_kwargs\n",
    "        )\n",
    "\n",
    "        self.valid_generator = valid_datagenerator.flow_from_directory(\n",
    "            directory=self.config.training_data,\n",
    "            subset=\"validation\",\n",
    "            shuffle=False,\n",
    "            **dataflow_kwargs\n",
    "        )\n",
    "\n",
    "        print(\"Valid Class Indices:\", self.valid_generator.class_indices)\n",
    "        print(\"Number of classes:\", len(self.valid_generator.class_indices))\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(path: Path) -> tf.keras.Model:\n",
    "        model = tf.keras.models.load_model(path)\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate the model on validation data.\"\"\"\n",
    "        self.model = self.load_model(self.config.path_of_model)\n",
    "        self._valid_generator()\n",
    "\n",
    "        num_classes = len(self.valid_generator.class_indices)\n",
    "        if num_classes != 4:\n",
    "            raise ValueError(f\"Expected 4 classes, found {num_classes}. Check your dataset!\")\n",
    "\n",
    "        self.score = self.model.evaluate(self.valid_generator)\n",
    "        print(f\"Loss: {self.score[0]}, Accuracy: {self.score[1]}\")\n",
    "        self._save_score()\n",
    "\n",
    "    def _save_score(self):\n",
    "        \"\"\"Save evaluation metrics to JSON.\"\"\"\n",
    "        scores = {\"loss\": self.score[0], \"accuracy\": self.score[1]}\n",
    "        with open(Path(\"scores.json\"), \"w\") as f:\n",
    "            json.dump(scores, f, indent=4)\n",
    "\n",
    "    def log_into_mlflow(self):\n",
    "        \"\"\"Log metrics and model to MLflow.\"\"\"\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            # Log hyperparameters\n",
    "            mlflow.log_params(self.config.all_params)\n",
    "            # Log metrics\n",
    "            mlflow.log_metrics({\"loss\": self.score[0], \"accuracy\": self.score[1]})\n",
    "            # Log model\n",
    "            mlflow.keras.log_model(self.model, \"model\") if tracking_url_type_store != \"file\" else mlflow.keras.log_model(self.model, \"model\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b58f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-25 19:36:35,545: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2025-09-25 19:36:35,547: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-09-25 19:36:35,547: INFO: common: created directory at: artifacts]\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 100356    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14815044 (56.51 MB)\n",
      "Trainable params: 100356 (392.02 KB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n",
      "Found 182 images belonging to 4 classes.\n",
      "Valid Class Indices: {'adenocarcinoma_left.lower.lobe_T2_N0_M0_Ib': 0, 'large.cell.carcinoma_left.hilum_T2_N2_M0_IIIa': 1, 'normal': 2, 'squamous.cell.carcinoma_left.hilum_T1_N2_M0_IIIa': 3}\n",
      "Number of classes: 4\n",
      "12/12 [==============================] - 1s 109ms/step - loss: 0.3790 - accuracy: 0.8681\n",
      "Loss: 0.37896257638931274, Accuracy: 0.8681318759918213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/25 19:36:39 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-25 19:36:40,531: INFO: builder_impl: Assets written to: /tmp/tmpkxlpoyz9/model/data/model/assets]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/09/25 19:36:44 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run caring-chimp-800 at: https://dagshub.com/AryanDhanuka10/End_to_End_Chest_Cancer_Detection_ML_Project_using_DVC_and_MLflow.mlflow/#/experiments/0/runs/a8eca22590fc439293f6e9d5d3af3d81\n",
      "🧪 View experiment at: https://dagshub.com/AryanDhanuka10/End_to_End_Chest_Cancer_Detection_ML_Project_using_DVC_and_MLflow.mlflow/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    eval_config = config.get_evaluated_config()\n",
    "    evaluation = Evaluation(eval_config)\n",
    "    evaluation.evaluate()\n",
    "    evaluation._save_score()\n",
    "    evaluation.log_into_mlflow()\n",
    "except Exception as e:\n",
    "    raise e\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
